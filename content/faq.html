
<div id="back_link">&laquo;&nbsp;<a href="javascript:load_content('docs')">Back to Documentation</a></div>

<h1>FAQ</h1>
<br/>


<h3>General Singularity info</h3>
<ul class="list-unstyled">

<li>
<a href="#thename" data-toggle="collapse">Why the name "Singularity"?</a>
</li>

<div id="thename" class="collapse dpdiv">
<p>
The name "Singularity" for me (Greg) stems back from my past experience
working at a company called <a href="https://en.wikipedia.org/wiki/Linuxcare">Linuxcare</a>
where the Linux Boot-able Business Card (LNX-BBC) was developed. The BBC, was
a Linux rescue disk which paved the way for all live CD bootable
distributions using a loop back file system called the "singularity".
</p>
<p>
This nomenclature represented that all files within the environment were
contained within a single file, and for the same reason Singularity
emphasizes the same nomenclature. (Thanks LNX-BBC!!!)
</p>
</div>


<li><a href="#namespaces" data-toggle="collapse">Which namespaces are virtualized? Is that select-able?</a></li>


<div id="namespaces" class="collapse dpdiv">
<p>
The goal of Singularity is to run an application within a contained
environment such as it was not contained. Thus there is a balance between
what to separate and what not to separate. At present the virtualized
namespaces are process, mount points, and certain parts of the contained
file system.
</p>
<p>
When you run your Singularity container, you may find that the process
IDs start with 1 (one) and increment from there. You will also find that
while the file system is contained starting with '/' (root), you can have
access outside the container via your starting path. This means that
relative paths will resolve outside the container, and fully qualified
paths will resolve inside the container.
</p>
<p>
To achieve this behavior, you will find that several Linux namespaces are
separated (PIDS, file systems and descriptors, mounts, and root file
system). These can be enabled or disabled by the build and what namespaces
the host system supports as well as through environment variables.
</p>
</div>


<li><a href="#runc" data-toggle="collapse">Why can't you just use RunC or any other container system on a shared system?</a></li>

<div id="runc" class="collapse dpdiv">
<p>
This is a copy/paste from a discussion on the email list describing RunC Vs.
Singularity on a shared HPC system.. But most of these tenants apply to all
of the popular container systems.
</p>
<p>
There are a number of reasons why RunC will not work on (my) shared
multi-tenant environments (your system may vary):
<ul class="list-unstyled">
<li>Requires root to run (there is however a submitted patch to allow non-root, but it has not been accepted at this point)</li>
<li>Even with the proposed patch, no mitigation of user escalation within the container</li>
<li>The container files themselves are owned by root, thus a user can not "bring their own environment"</li>
<li>No facility or optimization's for MPI or parallel job launch</li>
<li>Requires a very recent host operating system (RHEL7 and compats, and similar vintage Debian derivatives)</li>
<li>No automatic resolution of which namespaces to use (e.g. automatic disable PID namespace separation for OMPI shared memory optimization's)</li>
<li>It is not a "mobility of compute" solution (it is an example implementation of the OCI)</li>
<li>Users can escalate to root and potentially get access to shared file systems, run daemons and escape the standard user and scheduler limitations</li>
</ul>
</p>
<p>
Singularity on the other hand addresses these issues and more:
<ul class="list-unstyled">
<li>Singularity runs as the user that invoked it, and it prevents escalation pathways to obtain root within a container</li>
<li>Singularity can be used without any modification within an HPC environment (resource managers, interact with HPC file systems, interconnects, GPUs, etc..)</li>
<li>Because Singularity uses a single file for the container, that single file can be owned by a user but contain root owned files inside (thus a user can copy from another system)</li>
<li>Single file also optimizes parallel runs with lots of open()s (large python runs can take 10-30 minutes to start on a big system, but not in a container image)</li>
<li>Designed for: mobility/portability, speed, HPC, application virtualization (running apps within the container as if they are running on the host)</li>
<li>Works on all currently maintained vintages of Linux (e.g. RHEL 5 and compats)</li>
<li>No limitations on vintage of Container OS (e.g. I have a 17 year old install (RHL8) running in a Singularity image)</li>
</ul>
</p>
</div>

<li><a href="#docker" data-toggle="collapse">How does Singularity relate/differ from Docker?</a></li>

<div id="docker" class="collapse dpdiv">
<p>
Docker has been used for a variety of purposes, but it is designed as a
platform to provide replicatable, network service vitalization. Because of
this basic assumption and design model, it makes it difficult to implement
on shared HPC platforms (and thus Singularity was born). Additionally,
Docker supports the notion of emulating full operating system environments
including user context escalation.
</p>
<p>
Singularity on the other hand does not support user escalation or context
changes, nor does it have a root owned daemon process managing the
container namespaces. It also exec's the process work-flow inside the
container and seamlessly redirects all IO in and out of the container
directly between the environments. This makes doing things like MPI, X11
forwarding, and other kinds of work tasks trivial for Singularity.
</p>
<p>
If you already have a Docker container you can
<a href="#docker-import">import it directly into
Singularity</a>!
</p>
</div>


<li><a href="#runc1" data-toggle="collapse">How does Singularity relate/differ from RunC?</a></li>

<div id="runc1" class="collapse dpdiv">
<p>
This is a copy/paste from a discussion on the email list describing RunC Vs.
Singularity on a shared HPC system.. But most of these tenants apply to all
of the popular container systems.
</p>
<p>
There are a number of reasons why RunC will not work on (my) shared
multi-tenant environments (your system may vary):
<ul class="list-unstyled">
<li>Requires root to run (there is however a submitted patch to allow non-root, but it has not been accepted at this point)</li>
<li>Even with the proposed patch, no mitigation of user escalation within the container</li>
<li>The container files themselves are owned by root, thus a user can not "bring their own environment"</li>
<li>No facility or optimization's for MPI or parallel job launch</li>
<li>Requires a very recent host operating system (RHEL7 and compats, and similar vintage Debian derivatives)</li>
<li>No automatic resolution of which namespaces to use (e.g. automatic disable PID namespace separation for OMPI shared memory optimization's)</li>
<li>It is not a "mobility of compute" solution (it is an example implementation of the OCI)</li>
<li>Users can escalate to root and potentially get access to shared file systems, run daemons and escape the standard user and scheduler limitations</li>
</ul>
</p>
<p>
Singularity on the other hand addresses these issues and more:
<ul class="list-unstyled">
<li>Singularity runs as the user that invoked it, and it prevents escalation pathways to obtain root within a container</li>
<li>Singularity can be used without any modification within an HPC environment (resource managers, interact with HPC file systems, interconnects, GPUs, etc..)</li>
<li>Because Singularity uses a single file for the container, that single file can be owned by a user but contain root owned files inside (thus a user can copy from another system)</li>
<li>Single file also optimizes parallel runs with lots of open()s (large python runs can take 10-30 minutes to start on a big system, but not in a container image)</li>
<li>Designed for: mobility/portability, speed, HPC, application virtualization (running apps within the container as if they are running on the host)</li>
<li>Works on all currently maintained vintages of Linux (e.g. RHEL 5 and compats)</li>
<li>No limitations on vintage of Container OS (e.g. I have a 17 year old install (RHL8) running in a Singularity image)</li>
</ul>
</p>
</div>

<li><a href="#shifter" data-toggle="collapse">How does Singularity relate/differ from Shifter?</a></li>

<div id="shifter" class="collapse dpdiv">
<p>
NERSC (like most HPC centers) are feeling the pressure from users asking
for support for containers, specifically Docker. Due to the architecture
of Docker it is very difficult (if not impossible) to properly and
securely implement in a multi-tenant HPC environment. Shifter is NERSC's
implementation to provide a Docker compatible front-end interface to
their extreme scale HPC resources. It is system/resource specific in
that you must import an existing container (from Docker, Singularity,
or other), to the host/Shifter implementation.
</p>
<p>
Singularity on the other hand does not leverage the Docker work-flow and
targets a different premise - Mobility of Compute. This makes the
integration of Singularity non-HPC specific (even though it works very
well with HPC) and allows the image to become the primary unit of
mobility (you can share and operate directly on Singularity images).
</p>
<p>
Singularity is more of a general purpose mobility of compute solution
that is very capable at HPC, Shifter's primary focus is targeting extreme
scale HPC and integration with Cray and the resource manager.
</p>
</div>

<li><a href="#flatpak" data-toggle="collapse">How does Singularity relate/differ from Flatpak</a></li>

<div id="flatpak" class="collapse dpdiv">
<p>
Flatpak is a packaging subsystem that uses some container technologies to
create distribution neutral packages and it is more similar to the initial
proof of concept of Singularity. But the use-cases of Singularity dictated
that we should support full operating system containers that contain the
entire user's environment.
</p>
</div>


<li><a href="#other-containers" data-toggle="collapse">How does Singularity relate/differ from other container systems like OpenVz, LXC/LXD, etc.?</a></li>

<div id="other-containers" class="collapse dpdiv">
<p>
Singularity differs from other container systems in several major ways
that impact usability on shared systems. For example, most container
systems emulate standard systems in that there is the ability and
necessity to escalate to root, run on separate IP/network address,
run services, and in some cases even virtually boot the container system.
</p>
<p>
Singularity on the other hand focuses on the ability to virtualized only
what is necessary to achieve run-time application container and portable
environments. For instance, you can not obtain root from within a
Singularity container.
</p>
<p>
There are some additional performance and design enhancements which make
Singularity also more appropriate in a scheduled HPC environment. The
back-end image type is one such feature that negates the need for temporary
caching of container images and optimizes meta-data IO (especially on
parallel file systems). Another feature is how Singularity interacts with
the host operating system to facilitate application work-flows like X11
and MPI.
</p>
</div>

<li><a href="#static_bin" data-toggle="collapse">How does Singularity relate/differ from statically compiled binaries?</a></li>

<div id="static_bin" class="collapse dpdiv">
<p>
Statically compiled binaries are a good comparison to what Singularity can do
for a single program because it will package up all of the dynamic libraries
and package them into a single executable (interpreted) format.
</p>
<p>
But because Singularity is actually wrapping operating system files in to a
container, you can do much more with it... Such as include other files,
scripts, work-flows, pipe lines, data, and multi program processes and package
them into a single portable executable format.
</p>
</div>

<li><a href="#distros" data-toggle="collapse">What Linux distributions are you trying to get on-board?</a></li>
</ul>

<div id="distros" class="collapse dpdiv">
<p>
All of them! Help us out by letting them know you want Singularity to be
included!
</p>
</div>

<h3>Basic Singularity usage</h3>
<ul class="list-unstyled">
<li><a href="#admin" data-toggle="collapse">Do you need administrator privileges to use Singularity?</a></li>

<div id="admin" class="collapse dpdiv">
<p>
You do not need admin/sudo to use Singularity containers. You do however
need admin/root access to install Singularity and to build/manage your
containers and images, but to use the containers you do not need any
additional privileges to run programs within it.
</p>
<p>
This then defines the work-flow to some extent... Singularity container images
must be built and configured on a host where you have root access (this can
be a physical system or on a VM or Docker image). Once the container image
has been configured it can be used on a system where you do not have root
access as long as Singularity has been installed there.
</p>
</div>

<li><a href="#writable" data-toggle="collapse">Can you edit/modify a Singularity container once it has been instantiated?</a></li>

<div id="writable" class="collapse dpdiv">
<p>
Yes, if you call it with the -w/--writable flag. (e.g. 'singularity shell --writable Container.img').
</p>
</div>


<li><a href="#multiple_apps" data-toggle="collapse">Can multiple applications be packaged into one Singularity Container?</a></li>

<div id="multiple_apps" class="collapse dpdiv">
<p>
Yes! You can even create entire pipe lines and work flows using many
applications, binaries, scripts, etc.. Look into the RunScript bootstrap
definition option to define what happens when a Singularity container is
run (note: you can accomplish this by also creating an executable file
within your container at /singularity and when the container is executed
directly or via the 'run' command, this will get executed).
</p>
</div>

<li><a href="#binds" data-toggle="collapse">How are external file systems and paths handled in a Singularity Container?</a></li>

<div id="binds" class="collapse dpdiv">
<p>
Because Singularity is based on container principals, when an application
is run from within a Singularity container its default view of the file
system is different from how it is on the host system. This is what allows the
environment to be portable. This means that root ('/') inside the container
is different from the host!
</p>
<p>
Singularity automatically tries to resolve directory mounts such that things
will just work and be portable with whatever environment you are running on.
This means that /tmp and /var/tmp are automatically shared into the container
as is /home. Additionally, if you are in a current directory that is not a 
system directory, Singularity will also try to bind that to your container.
</p>
<p>
There is a caveat in that a directory *must* already exist within your
container to serve as a mount point. If that directory does not exist,
Singularity will not create it for you! You must do that.
</p>
</div>

<li><a href="#paths" data-toggle="collapse">What is the difference between full and relative paths?</a></li>

<div id="paths" class="collapse dpdiv">
<p>
See the above answer to "How are external file-systems and paths handled
in a Singularity Container?".
</p>
</div>

<li><a href="#networking" data-toggle="collapse">How does Singularity handle manage networking?</a></li>
 
<div id="networking" class="collapse dpdiv">
<p>
Singularity does no network isolation because it is designed to run like any
other application on the system. It has all of the same networking privileges
as any program running as that user.
</p>
</div>

<li><a href="#docker-import" data-toggle="collapse">Can I import an image from Docker?</a></li>
</ul>

<div id="docker-import" class="collapse dpdiv">
<p>
Yes, Docker has the ability to export the data of a particular container
and Singularity has the ability to import using the same format that Docker
exports. In a nutshell, it is as easy as:
</p>
<pre class="padPre text-justify">
$ docker export [container name] | sudo singularity import /path/to/container.img
</pre>
</div>


<h3>Contained applications and work flows</h3>
<ul class="list-unstyled">
<li><a href="#threads" data-toggle="collapse">Can a Singularity container be multi-threaded?</a></li>

<div id="threads" class="collapse dpdiv">
<p>
Yes. Singularity imposes no limitations on forks, threads or processes in
general.
</p>
</div>

<li><a href="#suspend" data-toggle="collapse">Can a Singularity container be suspended or check-pointed?</a></li>

<div id="suspend" class="collapse dpdiv">
<p>
Yes and maybe respectively. Any Singularity application can be suspended using
standard Linux/Unix signals. Check-pointing requires some preloaded libraries
to be automatically loaded with the application but because Singularity escapes
the hosts library stack, the checkpoint libraries would not be loaded. If
however you wanted to make a Singularity container that can be check-pointed,
you would need to install the checkpoint libraries into the Singularity
container via the specfile.
</p>
</div>

<li><a href="#sched" data-toggle="collapse">Are there any special requirements to use Singularity through a job scheduler?</a></li>

<div id="sched" class="collapse dpdiv">
<p>
Singularity containers can be run via any job scheduler without any
modifications to the scheduler configuration or architecture. This is because
Singularity containers are designed to be run like any application on the
system, so within your job script just call Singularity as you would any
other application!
</p>
</div>

<li><a href="#shared_sys" data-toggle="collapse">Does Singularity work in multi-tenant HPC cluster environments?</a></li>

<div id="shared_sys" class="collapse dpdiv">
<p>
Yes! HPC was one of the primary use cases in mind when Singularity was
created.
</p>
<p>
Most people that are currently integrating containers on HPC resources do it
by creating virtual clusters within the physical host cluster. This precludes
the virtual cluster from having access to the host cluster's high performance
fabric, file systems and other investments which make an HPC system high
performance.
</p>
<p>
Singularity on the other hand allows one to keep the high performance in High
Performance Computing by containerizing applications and supporting a runtime
which seamlessly interfaces with the host system and existing environments.
</p>
</div>

<li><a href="#x11" data-toggle="collapse">Can I run X11 apps through Singularity?</a></li>

<div id="x11" class="collapse dpdiv">
<p>
Yes. This works exactly as you would expect it to.
</p>
</div>

<li><a href="#mpi-hpc" data-toggle="collapse">Can I containerize my MPI application with Singularity and run it properly on an HPC system?</a></li>

<div id="mpi-hpc" class="collapse dpdiv">
<p>
Yes! HPC was one of the primary use cases in mind when Singularity was
created.
</p>
<p>
While we know for a fact that Singularity can support multiple MPI
implementations, we have spent a considerable effort working with Open MPI
as well as adding a Singularity module into Open MPI (v2) such that running
at extreme scale will be as efficient as possible.
</p>
<p>
note: We have seen no major performance impact from running a job in a
Singularity container.
</p>
</div>

<li><a href="#mpirun" data-toggle="collapse">Why do we call 'mpirun' from outside the container (rather then inside)?</a></li>

<div id="mpirun" class="collapse dpdiv">
<p>
With Singularity, the MPI usage model is to call 'mpirun' from outside the
container, and reference the container from your 'mpirun' command. Usage would
look like this:
<pre class="padPre text-justify">
$ mpirun -np 20 singularity exec container.img /path/to/contained_mpi_prog
</pre>
</p>
<p>
By calling 'mpirun' outside the container, we solve several very complicated
work-flow aspects. For example, if 'mpirun' is called from within the container
it must have a method for spawning processes on remote nodes. Historically ssh
is used for this which means that there must be an sshd running within the
container on the remote nodes, and this sshd process must not conflict with the
sshd running on that host! It is also possible for the resource manager to
launch the job and (in Open MPI's case) the Orted processes on the remote
system, but that then requires resource manager modification and container
awareness.
</p>
<p>
In the end, we do not gain anything by calling 'mpirun' from within the
container except for increasing the complexity levels and possibly loosing out
on some added performance benefits (e.g. if a container wasn't built with the
proper OFED as the host).
</p>
<p>
See the
<a href="javascript:load_content('hpc')">Singularity on HPC</a>
page for more details.
</p>
</div>


<li><a href="#gpu" data-toggle="collapse">Does Singularity support containers that require GPUs?</a></li>
</ul>

<div id="gpu" class="collapse dpdiv">
<p>
Yes, Singularity has been tested to run some test and diagnostic code from
within a container without modification. There are however potential issues
that can come into play when using GPUs, for instance there are version API
compatibilities between kernel and user land which will have to be considered.
</p>
</div>


<h3>Container portability</h3>
<ul class="list-unstyled">

<li><a href="#kernel_dependant" data-toggle="collapse">Are Singularity containers kernel dependent?</a></li>

<div id="kernel_dependant" class="collapse dpdiv">
<p>
No, never. But sometimes yes.
</p>
<p>
Singularity is using standard container principals and methods so if you
are leveraging any kernel version specific or external patches/module
functionality (e.g. OFED), then yes there maybe kernel dependencies you will
need to consider.
</p>
<p>
Luckily most people that would hit this are people that are using Singularity
to inter-operate with an HPC (High Performance Computing) system where there
are highly tuned interconnects and file systems you wish to make efficient
use of. In this case, See the documentation of MPI with Singularity.
</p>
<p>
There is also some level of glibc forward compatibility that must be taken
into consideration for any container system. For example, I can take a 
Centos-5 container and run it on Centos-7, but I can not take a Centos-7
container and run it on Centos-5.
</p>
<p>
note: If you require kernel dependent features, a container platform is
probably not the right solution for you.
</p>
</div>

<li><a href="#glibc" data-toggle="collapse">Can a Singularity container resolve GLIBC version mismatches?</a></li>
<div id="glibc" class="collapse dpdiv">
<p>
Yes. Singularity containers contain their own library stack (including the
Glibc version that they require to run).
</p>
</div>


<li><a href="#perf" data-toggle="collapse">What is the performance trade off when running an application native or through Singularity?</a></li>
</ul>

<div id="perf" class="collapse dpdiv">
<p>
So far we have not identified any appreciable regressions of performance (even
in parallel applications running across nodes with InfiniBand). There is a
small start-up cost to create and tear-down the container, which has been
measured to be anywhere from 10 - 20 thousandths of a second.
</p>
</div>


<h3>Misc</h3>
<ul class="list-unstyled">
<li><a href="#security" data-toggle="collapse">Are there any special security concerns that Singularity introduces?</a></li>


<div id="security" class="collapse dpdiv">
<p>
No and yes.
</p>
<p>
While Singularity containers always run as the user launching them, there are
some aspects of the container execution which requires escalation of privileges.
This escalation is achieved via a SUID portion of code. Once the container
environment has been instantiated, all escalated privileges are dropped
completely, before running any programs within the container.
</p>
<p>
Additionally, there are precautions within the container context to mitigate
any escalation of privileges. This limits a user's ability to gain root control
once inside the container.
</p>
<p>
You can read more about the
<a href="javascript:load_content('security')">Singularity security overview here</a>.
</p>
</div>

<li><a href="#swallow" data-toggle="collapse">What is the average air speed velocity of an unladen swallow?</a></li>
</ul>

<div id="swallow" class="collapse dpdiv">
<p>
Thanks Jason!
<a target="_blank" href="http://style.org/unladenswallow/">http://style.org/unladenswallow/</a>
</p>
</div>

<h3>Troubleshooting</h3>
<ul class="list-unstyled">
<li><a href="#grsec" data-toggle="collapse">How to use Singularity with GRSecurity enabled kernels</a></li>

<div id="grsec" class="collapse dpdiv">
<p>
To run Singularity on a GRSecurity enabled kernel, you must disable several
security features:

<pre class="padPre text-justify">
$ sudo sysctl -w kernel.grsecurity.chroot_caps=0
$ sudo sysctl -w kernel.grsecurity.chroot_deny_mount=0
$ sudo sysctl -w kernel.grsecurity.chroot_deny_chmod=0
$ sudo sysctl -w kernel.grsecurity.chroot_deny_fchdir=0
</pre>

</p>
</div>

<li><a href="#sudoerror" data-toggle="collapse">Error running Singularity with sudo</a></li>
</ul>

<div id="sudoerror" class="collapse dpdiv">
<p>
This fix solves the following error when Singularity is installed into the
default compiled prefix of /usr/local:
<pre class="padPre text-justify">
$ sudo singularity create /tmp/centos.img
sudo: singularity: command not found
</pre>
</p>
<p>
The cause of the problem is that `sudo` sanitizes the PATH environment
variable and does not include /usr/local/bin in the default search path.
Considering this program path is by default owned by root, it is
reasonable to extend the default sudo PATH to include this directory.
</p>
<p>
To add /usr/local/bin to the default sudo search path, run the program
<b>visudo</b> which will edit the sudoers file, and search for the
string 'secure_path'. Once found, append <b>:/usr/local/bin</b> to that
line so it looks like this:
<pre class="padPre text-justify">
Defaults    secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin
</pre>
</p>
</div>
